---
title: "Predicting the Winner of NBA's 2021 Most Improved Player Award, and Finding Candidates for 2022"
author: "Sumitro Datta"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```

# Re-Introduction

For my HarvardX capstone project, I performed an analysis on whether I could predict last year's Most Improved Player and this year's candidates (found here). With the 2020-2021 regular season in the books, I decided to go back and improve on it. Major changes include:

* a more robust dataset (instead of using the IMPORTHTML function on Google Sheets multiple times, I learned how to webscrape)
* a new variable (experience or seasons played in the league)
* different machine learning models (a decision tree does not lend itself to use with this many granular decimals)

# Introduction

As the beginning of a new NBA season nears, predictions start to roll in from pundits. In terms of major player awards, the Most Improved Player is the most "predicted by gut feeling".

* there are usually a select group of players that have a chance at Most Valuable Player and Defensive Player of the Year, with minor turnover year-over-year
* Sixth Man of the Year (given to the best performing player that does not start the game on the court) has morphed into a type: the high scoring guard that ironically often places in the top 5 in minutes played on the team
    + a guard has won 13 of the past 15 years, and the winner regardless of position has placed in the top 5 in minutes played on the team 12 of the 15 years
* Rookie of the Year, by definition, has turnover every year, but predictions tend towards players taken near the beginning of the draft

The goal of this analysis is to use machine learning to make a two-fold prediction.

1. Predict the winner of this year's (2021) MIP award.
2. Predict candidates of next year's (2022) MIP award.

On May 25, 2021, Julius Randle was revealed as the 2021 MIP. While disappointed I wasn't able to create a model in time, it'll serve as a good check.

Steps that were performed:

1. loading the data and swapping NA's with 0's
2. visualizing how MIP winners have performed and how much vote share they received
3. create a version of the data that shows statistical jumps (current season minus previous season)
4. separating out evaluation sets (the 2020 season) and test sets (the 1987 season for predicting the 2020 winner, the 1986 and 2019 seasons for predicting 2021 candidates)
5. training five models: a linear baseline, a k-nearest-neighbors model, two separate random forest models and a support vector machine model

Note: to simplify the Season column, I refer to each season by its second half. So you might also hear the current season referred to as the 2020-2021 season.

# Methods/Analysis

## Loading Packages

Let's start by loading required packages.
```{r load_pkgs, results="hide"}
if(!require(tidyverse)) 
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tidymodels)) 
  install.packages("tidymodels", repos = "http://cran.us.r-project.org")
#for variable importance
if(!require(vip)) 
  install.packages("vip", repos = "http://cran.us.r-project.org")
#Rborist and ranger are random forest algorithm wrappers
if(!require(Rborist)) 
  install.packages("Rborist", repos = "http://cran.us.r-project.org")
if(!require(ranger)) 
  install.packages("ranger", repos = "http://cran.us.r-project.org")
#matrix stats
if(!require(matrixStats)) 
  install.packages("matrixStats", repos = "http://cran.us.r-project.org")
#kableExtra allows more customization of tables
if(!require(kableExtra)) 
  install.packages("kableExtra")
if(!require(RColorBrewer)) 
  install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
#for dark background plots
if(!require(ggdark)) 
  install.packages("ggdark", repos = "http://cran.us.r-project.org")
```

## Downloading  the Data

This is a dataset that I have compiled from Basketball-Reference. Each player season contains some identifying factors:

* a unique Season ID
* the player's name
* the player's age as of February 1st of that season (per Basketball-Reference recordkeeping)
* the team that the player played for (if they played for multiple teams, it was recorded as TOT)
* the actual season
* the birth year
    + this is to break ties, as there are instances of 2 players having the same name and playing in the same season as well as father and son playing in different eras under the same name
* the number of years of experience the player has accumulated
    + a rookie, or a player with no prior experience, is given the value 1
    + this value is incremented up every subsequent season that a player has played in; if a player is a rookie in the 1990 NBA season, skips the 1991 NBA season and comes back for the 1992 NBA season, his experience value in 1992 is 2, not 3

There are three comma-separated value files of player statistics: 

* Player Per Game contains per game stats
* Per 100 Poss contains stats per 100 possessions
    + this adjusts for pace, which is the number of possessions teams use in 48 minutes (the typical length of a game, excluding overtimes)
    + this is necessary because the late 1990s/early 2000s were notoriously slow, so raw statistics were depressed
* Advanced contains advanced stats
    + for more information, refer to this [*Basketball-Reference glossary*](https://www.basketball-reference.com/about/glossary.html)
    
The above three files will be combined into one dataframe. For players that played for multiple teams in one season, the partial seasons will be discarded. The shooting percentage columns will by multiplied by 100 to remain consistent with the other percent columns. Lastly, we will take out columns that are the result of linear combinations of other columns:

* field goals made are the sum of 2-point field goals made & 3-point field goals made, similarly with attempts
* total rebounds are the sum of defensive rebounds & offensive rebounds
* win shares & box plus-minus, two holistic advanced statistics, are the sum of their offensive & defensive components

```{r load_stats_csvs,results='hide'}
#specify columns because otherwise birth year is read as logical
cols_for_stats=cols(
  .default = col_double(),
  player = col_character(),
  hof = col_logical(),
  pos = col_character(),
  lg = col_character(),
  tm = col_character()
)

advanced=read_csv("Data/Advanced.csv",col_types = cols_for_stats)
per_poss=read_csv("Data/Per 100 Poss.csv",col_types = cols_for_stats)
per_game=read_csv("Data/Player Per Game.csv",col_types = cols_for_stats)
combined=left_join(per_game,per_poss) %>% left_join(.,advanced) %>%
  mutate(tm=ifelse(tm=="TOT","1TOT",tm)) %>% 
  group_by(player_id,season) %>% arrange(tm) %>% slice(1) %>% 
  mutate(tm=ifelse(tm=="1TOT","TOT",tm)) %>% arrange(season,player) %>%
  mutate(across(c(fg_percent,x3p_percent,x2p_percent,e_fg_percent,ft_percent,ts_percent),
                ~.*100)) %>%
  select(-c(starts_with("fg_per_"),starts_with("fga_per_"),starts_with("trb_per_"),ws,bpm))
```

Anotther file that will be loaded is the Player Award Shares comma-separated values file. This file keeps track of both the number of first-place votes and the share of an award that a player received. Share equals the amount of points received divided by the maximum number of points possible. The file has multiple awards so it will be filtered for only MIP results. Up to the 2002 season, MIP voters were given one vote, and the player with the highest number of votes won the award. Starting in 2003, a point system was introduced, where:

* a first-place vote equals five points
* a second-place vote equals three points
* a third-place vote equals one point
* the player with the most ***points*** won the award

Theoretically, it is possible to have the most first-place votes and not win the award, but it has never happened in the history of the award.

```{r load_award_share_csvs,results='hide'}
mip_share=read_csv("Data/Player Award Shares.csv",col_types = cols(
  .default = col_double(),
  award= col_character(),
  player = col_character(),
  pos = col_character(),
  tm = col_character(),
  winner = col_logical() 
)) %>% filter(award=="mip") %>% select(season,seas_id:tm,share)
```

While the MIP share is tracked for the season in which the vote was received, it will also be tracked for the season prior. The latter column will help identify candidates for next season. For example, Pascal Siakam of the Toronto Raptors won the MIP award in 2019 with an MIP vote share of 0.938. This would be recorded in Siakam's 2019 season under `share` and in Siakam's 2018 Season under `share_next_seas`.

```{r add_share_next_seas,results='hide'}
stats_and_share=left_join(combined,mip_share) %>% group_by(player_id) %>% arrange(season) %>%
  mutate(share_next_seas=lead(share)) %>% ungroup()
```

The last file that will be loaded is the All-Star Selections comma-separated values file. I noticed last year that established players were factoring into my models more than I'd like. These players were under-performing due to injury and returning to their regular star level a year later. The number of All-Star selections a player has had prior to the current season can act as a proxy to a player's regular talent level. After joining the All-Star data with the statistics and vote share data, our last transformation is to remove seasons prior to 1984-1985 (which is one season before the inaugural awarding of the MIP award).

```{r load_all_star,results='hide'}
all_stars=read_csv("Data/All-Star Selections.csv") %>% mutate(is_allstar=1) %>%
  arrange(season,player) %>% group_by(player) %>% 
  mutate(num_allstars_inclu_seas=cumsum(is_allstar)) %>% ungroup() %>% 
  select(-c(team,replaced,hof,is_allstar))
final_data=left_join(stats_and_share,all_stars) %>% group_by(player_id) %>%
  #move all-star appearance to next season
  mutate(all_stars_lag=ifelse(!is.na(lag(num_allstars_inclu_seas)),
                               lag(num_allstars_inclu_seas),NA)) %>%
  #add zero for easier filling of NA's
  mutate(all_stars_tot=ifelse(lead(all_stars_lag)==1,0,NA)) %>%
  #combine two previous columns
  mutate(num_allstars_before_seas=coalesce(all_stars_lag,all_stars_tot)) %>%
  fill(num_allstars_before_seas,.direction="downup") %>% ungroup() %>%
  filter(season > 1984) %>%
  select(-c(all_stars_lag,all_stars_tot,num_allstars_inclu_seas))
```
Let's see if there is any missing data.
```{r see_missing}
colSums(is.na(final_data))
```

Looks like there's a significant amount of missing data, but it can be broken down into bite-size parts:

* the NA's in the shooting percentages are due to a lack of attempts
* the consistent 5 NA's in the per-100-possessions and advanced stats are due to a lack of data, as those 5 players played less than a minute of game time
  + the rest of the NA's in per-100-possessions & advanced can also be attributed to a lack of data, but rather than minutes played, it is the absence of the stat itself
* all the players who never received an MIP vote and were never an all-star have NA's
* all the players who don't need a birth year to separate them from another player with the same name also have an NA

The above NA's can all be converted to zeroes.

```{r remove_missing}
final_data[is.na(final_data)]<-0
```

```{r clean_environ,include=FALSE}
rm(advanced,per_game,per_poss,
   mip_share,combined,cols_for_stats,stats_and_share,all_stars)
```

## Data Exploration and Visualizations

We'll start by seeing how much vote share MIP award winners have received.
```{r mip_winner_shares, echo=FALSE}
mip_winners<-final_data %>% filter(season!=1987) %>% group_by(season) %>% 
  #get player names and season IDs from indexes
  summarize(player=player[which.max(share)],
            seas_id=seas_id[which.max(share)],
            share=max(share))
dale_ellis<-final_data %>% filter(season==1987,player=="Dale Ellis") %>%
  select(season,player,seas_id,share)
mip_winners <- bind_rows(mip_winners,dale_ellis) %>% arrange(season)
rm(dale_ellis)
kable(mip_winners[,c(1:2,4)],booktabs=TRUE) %>% kable_styling(position="center")
```

There are three seasons that have a vote share of zero: 1985 (which was the season before the first awarding of the MIP), 2021 (which is the season we are trying to predict for) and 1987.  

There was an MIP award given out in 1987 to Dale Ellis of the Seattle SuperSonics. However, when researching, I could not find any voting records for the season. I'll discuss what I plan to do with this season a little further in the report.

Secondly, we'll plot the scoring per game of the MIP winners. The three missing seasons (1985,1987,2020) have been removed.
```{r mip_ppg, fig.align='center',echo=FALSE,out.width="100%"}
# take out missing seasons
mip_winners <- mip_winners %>% filter (!(season %in% c(1985,2021)))
#remove repeated columns
mip_stats <- left_join(mip_winners,final_data)
ppg_mip <- mip_stats %>%
  #minus in fill so more intense correlates to higher points
  ggplot(aes(x=season,y=pts_per_game,fill=-pts_per_game)) + 
  geom_bar(stat="identity")+
  scale_fill_distiller(palette="YlOrRd")+
  #player label within bar, points per game label above bar
  geom_text(aes(label=player),size=3,angle="90",hjust=1.1,colour="black")+
  geom_text(aes(label=pts_per_game),size=2,vjust=-1,colour="white")+
  dark_theme_gray()+theme(legend.position = "none")
ggsave(filename = "Images/Points Per Game by MIP Winners.png",plot=ppg_mip,
       width=8,height=4.15,units = "in")
knitr::include_graphics("Images/Points Per Game by MIP Winners.png")
```
The lowest points per game average by an MIP is 9.7 (achieved by Isaac Austin in 1997), while the highest is 26.8 (achieved by Tracy McGrady in 2001).

Next, we'll plot the MIP winners by Win Shares per 48 minutes. Win Shares represent how much a player has contributed to his team's wins by comparing his output to a marginal player. Win Shares per 48 Minutes is a rate stat to compare how effective a player was during their time on the court, regardless of the actual minutes played.
```{r mip_ws_per_48, fig.align='center',echo=FALSE, out.width="100%"}
ws_mip<- mip_stats %>%
  #minus in fill so more intense correlates to higher ws/48
  ggplot(aes(x=season,y=ws_48,fill=-ws_48)) +
  geom_bar(stat="identity",position="dodge")+
  scale_fill_distiller(palette="YlOrRd")+
  #player label within bar, points per game label above bar
  geom_text(aes(label=player),size=3,angle="90",hjust=1.1,color="black")+
  geom_text(aes(label=round(ws_48,digits=2)),size=2,vjust=-1,color="white")+
  dark_theme_gray()+theme(legend.position = "none")
ggsave(filename = "Images/WS per 48 Mins by MIP Winners.png",plot=ws_mip,
       width=8,height=4.15,units = "in")
knitr::include_graphics("Images/WS per 48 Mins by MIP Winners.png")
```

The lowest win shares per 48 minutes by an MIP is 0.06 (achieved by Rony Seikaly in 1990), while the highest is 0.22 (achieved by Ryan Anderson in 2012).

Thirdly, we'll plot the experience of the winners.
```{r mip_experience,echo=FALSE, fig.align='center', out.width="100%"}
experience_over_time <- mip_stats %>% 
  ggplot(aes(season,experience))+geom_bar(stat="identity")+
  dark_theme_gray()+theme(legend.position = "none")
ggsave(filename = "Images/Experience of MIP Winners.png",plot=experience_over_time,
       width=8,height=4.15,units = "in")
knitr::include_graphics("Images/Experience of MIP Winners.png")
```
In the first few years of awards, second-year players ruled the roost, winning 4 of the first 5 awards. Recently, the trend has been to recognize a player in their third or fourth season.

Finally, we'll plot how the max voting share has evolved over the life of the award, placing a vertical line at the 2003 season when the voting system changed.
```{r mip_share, echo=FALSE, fig.align='center', out.width="100%"}
befo_2003_avg<-mip_stats %>% filter (season<2003 & season != 1987) %>% 
  summarize(mean(share))
after_2003_avg<-mip_stats %>% filter (season>=2003) %>% 
  summarize(mean(share))
share_over_time <- mip_stats %>% 
  ggplot(aes(season,share))+geom_bar(stat="identity")+
  geom_vline(xintercept = 2003,color="red",linetype="dotdash")+
  annotate("text",x=1990,y=0.75,
           label=paste("Mean before 2003: \n",
                       round(befo_2003_avg, digits = 3)))+
  annotate("text",x=2010,y=0.875,
           label=paste("Mean after 2003: \n",
                       round(after_2003_avg, digits = 3)))+
  dark_theme_gray()+theme(legend.position = "none")
ggsave(filename = "Images/Vote Share of MIP Winners.png",plot=share_over_time,
       width=8,height=4.15,units = "in")
knitr::include_graphics("Images/Vote Share of MIP Winners.png")
```

Looks like the average jumps up significantly after the voting system was changed. In recent years, there's been more consensus.

```{r remove_plots_and_related,include=FALSE}
rm(mip_stats,mip_winners,ppg_mip,ws_mip,experience_over_time,share_over_time,
   befo_2003_avg,after_2003_avg)
```

## Preprocessing

Before we fit our model, we should do some pre-processing. We've already done some pre-processing by handling missing values. 

For the 2021 award winner, we realize that players must have at least one prior season played in order to garner consideration. So we will subset out all players who have only one season played in the database. Removing one-season players has the unfortunate side effect on removing some players who played their last games in 1985, but had a career before then. This shouldn't be too worrisome, as no MIP has won in their final season in the league.

In addition, statistical jumps from the previous season play a much bigger part than the current season's statistics in determining the winner. A player going from 20 points per game to 21 points per game will not merit as much campaigning as a player going from 8 PPG to 18 PPG.

Finally, we will also remove some extraneous information from the data because we don't want to predict based off of a player's team or position.

```{r one_seasoners_and_jumps}
one_seasoners<-final_data %>% group_by (player_id) %>% 
  mutate(n=n_distinct(season)) %>% filter(n==1) %>% select(seas_id,player_id,player)
full_jumps_data<-final_data %>% 
  #remove the one-season players
  filter(!(seas_id %in% one_seasoners$seas_id)) %>%
  group_by(player_id) %>% 
  #arrange seasons from first to last within each player
  arrange(season,.by_group=TRUE) %>%
  #subtract prior season from current season
  mutate_at(.vars=c(colnames(final_data[12:68])),
            .funs=funs(`diff`=.-lag(.,default=first(.)))) %>%
  #do not keep the raw stats, only differences
  select(seas_id:tm, g_diff:pts_per_game_diff,x3p_per_100_poss_diff:vorp_diff,
         share,num_allstars_before_seas) %>% 
  #remove first season from every group (no difference)
  slice(-1) %>% arrange(seas_id) %>% ungroup() %>% 
  #do not keep irrelevant information
  select(seas_id:player,age:experience,g_diff:num_allstars_before_seas)
```

For predicting candidates in 2022, it's a much more diverse subset. So we'll just remove some of the irrelevant factors from the data.

```{r next_seas_dataframe}
full_data<-final_data %>% select(seas_id:player,age:experience,g:num_allstars_before_seas) %>%
  select(-c(mp,share))
```

## Creating an Evaluation Set

Our evaluation set is straightforward: it is players who have played in the 2021 season. So we segment it off here.
```{r evaluation_set_2021}
eval_2021<-full_data %>% filter (season==2021)
full_data<-full_data %>% filter (season != 2021)
eval_jumps_2021<-full_jumps_data %>% filter (season == 2021)
full_jumps_data<-full_jumps_data %>% filter (season != 2021)
```

As discussed earlier, I could not find any voting records for the 1987 season. All I could find was the winner (Dale Ellis of the Seattle SuperSonics). Instead of removing the 1987 data, I realize that I could use it as a test set or additional performance metric before applying the algorithm to find the 2020 season winner. Ideally, the algorithm would project Ellis to be the winner, and also provide me with the other candidates from that season. In the same vein, I'll cast the 1986 season and 2020 season as test sets in the journey to predict the 2021 candidates. By moving the test sets to the bottom of their respective datasets, I can use the `initial_time_split` function from tidymodels to segment them out.

```{r test_sets}
full_data_tidy<-full_data %>% 
  mutate(train_or_test=ifelse(season %in% c(1986,2020),"test","train")) %>% 
  arrange(desc(train_or_test)) %>% select(-train_or_test)
full_jumps_data_tidy<-full_jumps_data %>% 
  mutate(train_or_test=ifelse(season == 1987,"test","train")) %>% 
  arrange(desc(train_or_test)) %>% select(-train_or_test)

full_jumps_split=initial_time_split(
  full_jumps_data_tidy,prop=full_jumps_data_tidy %>% 
    summarize(test_prop=1-sum(season==1987)/n()) %>% pull()
  )
jumps_trainer=training(full_jumps_split)
jumps_tester=testing(full_jumps_split)

full_split=initial_time_split(
  full_data_tidy,prop=full_data_tidy %>% 
    summarize(test_prop=1-sum(season %in% c(1986,2020))/n()) %>% pull()
  )
trainer=training(full_split)
tester=testing(full_split)
```
```{r clean_environ2,include=FALSE}
rm(stats_and_share,one_seasoners,
   full_data,full_jumps_data,
   full_data_tidy,full_jumps_data_tidy)
```

## Training Models

### Models to Predict the 2021 Winner

With caret, everything could be thrown into the train function: cross-validation, tuning, data, etc. Tidymodels is more explicit in its steps. We'll start off with common steps across all models. The standard 10-fold cross validation is how the data is going to be resampled. Recipes is tidymodels' pre-processing package. The last pre-processing step is to remove the rest of the identifying information. We kept them initially so that it's easier to match predictions to players. We'll also be excluding the games difference and games started difference (g_diff and gs_diff respectively) as predictors because the NBA has had various fluctuations from the standard 82-game slate:
  
* the 1999 and 2012 seasons were started late due to lockouts
* the 2020 and 2021 seasons were condensed due to the COVID-19 pandemic

By including these predictors, we can only lower our estimate.

```{r common_2021_steps}
this_year_cv=vfold_cv(jumps_trainer,v=10)
this_year_recipe<-recipe(share ~ .,data=full_jumps_split) %>%
  step_rm(c(seas_id:player,g_diff,gs_diff))
```

Our first model is a simple linear regression, which will serve as a baseline.
```{r this_year_linear}
lin_model<-linear_reg() %>% set_engine("lm") %>% set_mode("regression")
this_year_workflow<-workflow() %>% add_recipe(this_year_recipe) %>%
  add_model(lin_model)
tuning<-this_year_workflow %>% 
  tune_grid(resamples=this_year_cv,metrics=metric_set(rmse))
this_year_workflow <-this_year_workflow %>% 
  finalize_workflow(tuning %>%select_best(metric="rmse"))
linear_this_yr=fit(this_year_workflow,jumps_trainer)
```
Let's get the most important variables of the linear model.
```{r linear_top_vars,echo=FALSE}
vip(pull_workflow_fit(linear_this_yr),scale=TRUE)
```
The most important variable is the difference in minutes per game. As a player gets more time on the court, there's more opportunity to rack up positive stats like rebounds and assists but also negative stats like personal fouls and turnovers. The new experience variable checks in as the 7th-most important variable in the linear variable, and the rate & total versions of win shares round out the top 10.

We'll evaluate performance by checking RMSE, the predicted vote share of Dale Ellis in the 1987 test set and the top 3 candidates in 1987. Remember, Ellis won the 1987 MIP but there were no voting records found during research.

```{r linear_1987}
resid_mean_sq_err=tuning %>% collect_metrics() %>% pull(mean)
pred_1987=bind_cols(jumps_tester %>% select(seas_id:experience),
               predict(linear_this_yr,new_data=jumps_tester)) %>% arrange(desc(.pred))
results_tib=tibble(Method="Linear",
                     RMSE=resid_mean_sq_err,
                     "Ellis Vote Share"=pred_1987 %>% 
                       filter(player=="Dale Ellis") %>% pull(.pred),
                     "Top 3 Candidates"=pred_1987 %>%
                       slice_max(.pred,n=3) %>% pull(player),
                     "Top 3 Vote Shares"=pred_1987 %>%
                       slice_max(.pred,n=3) %>% pull(.pred))
results_tib %>% knitr::kable()
```
The vote shares are quite low. Ellis places second behind Michael Jordan. This is somewhat misleading, because Jordan broke his foot the previous year and missed 64 games. He came back with a vengeance, upping his points per game by 14.4. But Jordan was already a star, and stars are implicitly excluded from winning MIP. I'd hoped that including the number of All-Star selections prior to the current season would alleviate this problem, but it wasn't a big factor in the linear model. Thorpe is a worthy candidate: he increased his scoring by nine points and WS/48 by 0.04 while decreasing his turnover percentage.
